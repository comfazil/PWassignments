{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb3f59a-7d0f-4495-b0dc-e7fd04225a56",
   "metadata": {},
   "source": [
    "## Q1. What is Bayes' theorem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5626cea8-b37a-46fc-94b8-0c7b7b5af59c",
   "metadata": {},
   "source": [
    "Bayes' theorem, also known as Bayes' rule or Bayes' law, is a fundamental concept in probability theory and statistics. It describes how we can update our beliefs or estimates about the probability of an event occurring based on new evidence or information. The theorem is named after the 18th-century English mathematician Thomas Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32edfffc-901b-45de-b407-073a9363e48d",
   "metadata": {},
   "source": [
    "## Q2. What is the formula for Bayes' theorem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a3a681-b61a-4e63-b1cf-ed9875156a3c",
   "metadata": {},
   "source": [
    "The formula for Bayes' theorem can be written as:\n",
    "\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "where:\n",
    "\n",
    "- P(A|B) is the conditional probability of A given B (the probability of A happening, given that B has occurred).\n",
    "- P(B|A) is the conditional probability of B given A (the probability of B happening, given that A has occurred).\n",
    "- P(A) is the prior probability of A (the probability of A happening before any new evidence is taken into account).\n",
    "- P(B) is the prior probability of B (the probability of B happening before any new evidence is taken into account)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e921fc-8061-43b6-8f06-096a108fa670",
   "metadata": {},
   "source": [
    "## Q3. How is Bayes' theorem used in practice?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3764c22f-90be-415b-ae4d-17cb90713fa3",
   "metadata": {},
   "source": [
    "Bayes' theorem is widely used in many fields, including science, engineering, finance, and medicine, to name a few. Some practical applications of Bayes' theorem include:\n",
    "\n",
    "Spam filtering: Bayes' theorem can be used to identify spam emails by calculating the probability of an email being spam given certain words or phrases in the email.\n",
    "Medical diagnosis: Bayes' theorem can be used to calculate the probability of a patient having a certain disease given their symptoms and medical history.\n",
    "Weather forecasting: Bayes' theorem can be used to predict the probability of certain weather conditions given past weather data and other relevant factors.\n",
    "Machine learning: Bayes' theorem is used in many machine learning algorithms, such as Naive Bayes classifiers, to make predictions based on data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c88bab4-cba9-4b23-aea0-39cd84d4333e",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between Bayes' theorem and conditional probability?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b26e463-38f5-499b-b323-0ab94170c9c4",
   "metadata": {},
   "source": [
    "Bayes' theorem is a way to calculate conditional probabilities. It provides a way to update our beliefs or estimates about the probability of an event occurring based on new evidence or information. The formula for Bayes' theorem includes conditional probabilities of the form P(A|B) and P(B|A)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01effad5-4bf2-4ff5-a91d-39099a1bf3ee",
   "metadata": {},
   "source": [
    "## Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c4ccb-7eed-4adc-b134-ff6d0c6f9910",
   "metadata": {},
   "source": [
    "There are three main types of Naive Bayes classifiers: Gaussian, Multinomial, and Bernoulli. The choice of which type to use depends on the nature of the data being analyzed.\n",
    "\n",
    "Gaussian Naive Bayes is suitable for continuous data that follows a normal distribution, such as height or weight measurements.\n",
    "Multinomial Naive Bayes is suitable for discrete data that has a count or frequency, such as word occurrences in text data.\n",
    "Bernoulli Naive Bayes is suitable for binary data, such as yes/no or true/false responses.\n",
    "In practice, the choice of Naive Bayes classifier can also depend on factors such as the size and quality of the data set, as well as the specific problem being addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd1a276-c20f-4bf4-a656-c345369b3b24",
   "metadata": {},
   "source": [
    "## Q6. You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of each feature value for each class:\n",
    " \n",
    "             Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "             \n",
    "             A     3 3 4 4 3 3 3\n",
    "             B     2 2 1 2 2 2 3\n",
    "\n",
    "## Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121a5d33-df89-46be-b1fe-01fe86d2e0ed",
   "metadata": {},
   "source": [
    "To classify the new instance with features X1=3 and X2=4 using Naive Bayes, we need to calculate the posterior probabilities for each class given these feature values.\n",
    "\n",
    "Using Bayes' theorem, we have:\n",
    "\n",
    "P(A|X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) / P(X1=3,X2=4)\n",
    "\n",
    "and\n",
    "\n",
    "P(B|X1=3,X2=4) = P(X1=3,X2=4|B) * P(B) / P(X1=3,X2=4)\n",
    "\n",
    "Since we have no prior knowledge or reason to believe one class is more likely than the other, we can assume that P(A) = P(B) = 0.5.\n",
    "\n",
    "To calculate P(X1=3,X2=4|A), we can use the frequency table and assume a multinomial distribution. Therefore, we have:\n",
    "\n",
    "P(X1=3,X2=4|A) = (4/17)^2 * (3/17)^2 * (4/10) = 0.002174\n",
    "\n",
    "Similarly, we can calculate P(X1=3,X2=4|B):\n",
    "\n",
    "P(X1=3,X2=4|B) = (1/5)^2 * (2/5)^2 * (3/10) = 0.002304\n",
    "\n",
    "To calculate P(X1=3,X2=4), we can use the law of total probability:\n",
    "\n",
    "P(X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) + P(X1=3,X2=4|B) * P(B)\n",
    "= 0.002174 * 0.5 + 0.002304 * 0.5\n",
    "= 0.002239\n",
    "\n",
    "Now, we can calculate the posterior probabilities for each class:\n",
    "\n",
    "P(A|X1=3,X2=4) = 0.002174 * 0.5 / 0.002239 = 0.486\n",
    "\n",
    "P(B|X1=3,X2=4) = 0.002304 * 0.5 / 0.002239 = 0.514\n",
    "\n",
    "Therefore, Naive Bayes would predict that the new instance belongs to class B, since it has a higher posterior probability.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd0a059-82e1-47e3-a027-a586eb48560a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
