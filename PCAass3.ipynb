{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3cf7873-f1d7-498d-9baa-1909107a5050",
   "metadata": {},
   "source": [
    "## Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach? Explain with an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55088801-3339-4384-8782-fd710853d1b4",
   "metadata": {},
   "source": [
    "Eigenvectors and eigenvalues are concepts from linear algebra used in the Eigen-Decomposition approach. An eigenvector is a non-zero vector that only changes in scale when a linear transformation is applied to it. An eigenvalue is a scalar value that represents the scale factor by which the eigenvector is scaled when the linear transformation is applied. The Eigen-Decomposition approach involves decomposing a matrix into a set of eigenvectors and eigenvalues.\n",
    "\n",
    "For example, consider the following matrix A:\n",
    "\n",
    "    2  1\n",
    "    1  2\n",
    "    \n",
    "We can find its eigenvectors and eigenvalues by solving the equation Av = λv, where A is the matrix, v is the eigenvector, and λ is the eigenvalue. Solving this equation yields two eigenvalues, λ1 = 3 and λ2 = 1, and two corresponding eigenvectors, v1 = [1, 1] and v2 = [-1, 1].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6937857-1ee3-4f05-80e3-b8892212ca76",
   "metadata": {},
   "source": [
    "## Q2. What is eigen decomposition and what is its significance in linear algebra?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a8d906-47ff-43b3-9739-dc486d2aaf6d",
   "metadata": {},
   "source": [
    " Eigen decomposition is the process of decomposing a matrix into a set of eigenvectors and eigenvalues. It is significant in linear algebra because it allows us to express a matrix in terms of its eigenvalues and eigenvectors, which can simplify many computations and transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6223eed6-4603-44b7-9997-048160bf2b1a",
   "metadata": {},
   "source": [
    "## Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the Eigen-Decomposition approach? Provide a brief proof to support your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23da7b0c-b9f9-4dd4-9b46-ae616c32252b",
   "metadata": {},
   "source": [
    "A square matrix can be diagonalizable using the Eigen-Decomposition approach if it satisfies the following conditions:\n",
    "\n",
    "- The matrix has n linearly independent eigenvectors.\n",
    "- The eigenvectors can form a basis for the vector space of the matrix.\n",
    "- A brief proof to support this is that if a square matrix A has n linearly independent eigenvectors, we can construct a matrix P whose columns are the eigenvectors of A. Then we can express A as A = PDP^-1, where D is a diagonal matrix of the eigenvalues of A. This shows that A is diagonalizable using the Eigen-Decomposition approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4305660f-6bbc-401c-82af-2371d495c424",
   "metadata": {},
   "source": [
    "## Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach? How is it related to the diagonalizability of a matrix? Explain with an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adde92a-596c-49ff-b9c8-48292a39d2b3",
   "metadata": {},
   "source": [
    " The spectral theorem states that a matrix A is diagonalizable if and only if it has n linearly independent eigenvectors. This is significant in the context of Eigen-Decomposition because it provides a necessary and sufficient condition for a matrix to be diagonalizable using the Eigen-Decomposition approach. For example, consider a symmetric matrix A. Since a symmetric matrix has n linearly independent eigenvectors, it is always diagonalizable using Eigen-Decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407d776c-1f7b-4a83-973d-7b3fb68d4d64",
   "metadata": {},
   "source": [
    "## Q5. How do you find the eigenvalues of a matrix and what do they represent?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04091333-e45d-488e-aa13-c9913dd11bff",
   "metadata": {},
   "source": [
    " Eigenvalues of a matrix can be found by solving the characteristic equation det(A-λI) = 0, where A is the matrix, λ is an eigenvalue, and I is the identity matrix. Eigenvalues represent the scaling factor by which the corresponding eigenvectors are scaled when the matrix transformation is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08899a96-ce99-484e-918f-1a235622afc2",
   "metadata": {},
   "source": [
    "## Q6. What are eigenvectors and how are they related to eigenvalues?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97723645-0729-4ef4-a4ab-9c1565ce6292",
   "metadata": {},
   "source": [
    " Eigenvectors are non-zero vectors that only change in scale when a linear transformation is applied to them. They are related to eigenvalues because an eigenvector multiplied by a matrix is equal to the same eigenvector scaled by the corresponding eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded4b800-d3fe-4590-8322-e142cd0c94c3",
   "metadata": {},
   "source": [
    "## Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e77dbba-8dbf-437f-b0b4-29dbc1aba710",
   "metadata": {},
   "source": [
    "Geometrically, eigenvectors represent the directions in a space that are preserved by a matrix transformation, while eigenvalues represent the scaling factor by which the transformation stretches or shrinks the eigenvectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1094bc-f6c8-41eb-ab27-9a59aaedadf8",
   "metadata": {},
   "source": [
    "## Q8. What are some real-world applications of eigen decomposition?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af586825-a04d-46ff-aae2-577060078543",
   "metadata": {},
   "source": [
    "Eigen decomposition has many real-world applications, including:\n",
    "\n",
    "- Image compression and processing\n",
    "- Principal Component Analysis (PCA) in machine learning\n",
    "- Solving differential equations and other mathematical problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f12b8e-68b7-4cbf-b776-69d1ad95385d",
   "metadata": {},
   "source": [
    "## Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e26750-0586-4af3-b29f-6ed48a4d2698",
   "metadata": {},
   "source": [
    "Yes, a matrix can have more than one set of eigenvectors and eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bafe35-8bdf-4771-aaf0-f79468e4ceb8",
   "metadata": {},
   "source": [
    "## Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning? Discuss at least three specific applications or techniques that rely on Eigen-Decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f3647b-d968-4901-90b9-e78d0dd3b9cd",
   "metadata": {},
   "source": [
    " The Eigen-Decomposition approach is useful in many applications, especially in data analysis and machine learning. Here are three specific applications or techniques that rely on Eigen-Decomposition:\n",
    "\n",
    "- Principal Component Analysis (PCA): As discussed earlier, PCA is a technique that uses Eigen-Decomposition to find the principal components of a dataset. By using Eigen-Decomposition to find the Eigenvectors and Eigenvalues of the covariance matrix, PCA can reduce the dimensionality of a dataset and identify the most important features or components.\n",
    "\n",
    "- Singular Value Decomposition (SVD): SVD is another technique that relies on Eigen-Decomposition. SVD factorizes a matrix into three matrices, including two orthogonal matrices and a diagonal matrix. The diagonal matrix contains the singular values of the original matrix, which are equivalent to the square roots of the Eigenvalues of the original matrix. SVD is used in many applications, such as image compression, recommendation systems, and natural language processing.\n",
    "\n",
    "- Graph Laplacian Eigenmaps: Graph Laplacian Eigenmaps is a technique used in machine learning and data analysis to embed a dataset into a lower-dimensional space while preserving its graph structure. It uses Eigen-Decomposition to find the Eigenvectors and Eigenvalues of the Laplacian matrix, which is derived from the adjacency matrix of the graph. The Eigenvectors corresponding to the smallest Eigenvalues are used as the embedding coordinates. Graph Laplacian Eigenmaps is used in many applications, such as image and video processing, social network analysis, and clustering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8ee3a-7631-4e7b-a9f7-d813f0e21166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
