{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad51e10-f38f-4ae2-b418-2c9914136ae0",
   "metadata": {},
   "source": [
    "# Q.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7768507-9887-4a14-ac51-ac0d20aaab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Define the feature selection step\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "\n",
    "# Define the numerical pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define the categorical pipeline\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# Combine the numerical and categorical pipelines using ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']),\n",
    "    ('cat', cat_pipeline, ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])\n",
    "])\n",
    "\n",
    "# Combine the feature selection and preprocessor using a pipeline\n",
    "feature_engineering_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('selector', selector)\n",
    "])\n",
    "\n",
    "# Define the final Random Forest Classifier pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('feature_engineering', feature_engineering_pipeline),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Split the dataset into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline on the test data\n",
    "accuracy = rf_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c4bb9d-e23b-4466-826e-74fbad7d6d9f",
   "metadata": {},
   "source": [
    "In this pipeline, we first define a feature selection step using the SelectKBest method with the f_classif score function. This step selects the top k=10 most important features based on their ANOVA F-value.\n",
    "\n",
    "Next, we define two separate pipelines for handling the numerical and categorical features. For the numerical pipeline, we use SimpleImputer to replace missing values with the mean of the column values, and StandardScaler to scale the features to have zero mean and unit variance. For the categorical pipeline, we use SimpleImputer to replace missing values with the most frequent value of the column, and OneHotEncoder to one-hot encode the categorical features.\n",
    "\n",
    "We then combine the numerical and categorical pipelines using the ColumnTransformer, which applies the appropriate pipeline to each column of the input data.\n",
    "\n",
    "Next, we combine the feature selection and preprocessor pipelines using another pipeline. This pipeline first applies the preprocessor to the input data to handle missing values and encode categorical features, and then selects the top 10 most important features using the SelectKBest method.\n",
    "\n",
    "Finally, we define the final pipeline by combining the feature engineering pipeline with a Random Forest Classifier. The Random Forest Classifier uses an ensemble of decision trees to predict the target variable.\n",
    "\n",
    "We then split the data into training and testing sets using train_test_split and train the pipeline on the training data. Finally, we evaluate the pipeline on the test data using the score method, which returns the accuracy of the model.\n",
    "\n",
    "To improve the pipeline, we could try tuning the hyperparameters of the Random Forest Classifier using GridSearchCV or RandomizedSearchCV. We could also experiment with different feature selection methods, preprocessing steps, or classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d808c69c-21a9-4ced-abfc-805f9c22c3f8",
   "metadata": {},
   "source": [
    "# Q. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6536df8e-d2a9-4281-ae61-c4a9be48ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the column transformer for preprocessing the data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the Random Forest Classifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# Define the Logistic Regression Classifier\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Define the Voting Classifier\n",
    "voting = VotingClassifier(estimators=[('rfc', rfc), ('lr', lr)], voting='hard')\n",
    "\n",
    "# Define the pipeline\n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('voting', voting)])\n",
    "\n",
    "# Fit the pipeline on the training set\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the accuracy of the pipeline on the test set\n",
    "y_pred = pipe.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabeb582-ce3b-4e5a-bdc2-bdfbeeda8455",
   "metadata": {},
   "source": [
    "In this pipeline, we first define a preprocessor that handles missing values and encodes categorical features using one-hot encoding. We then define a RandomForestClassifier and a LogisticRegression classifier, and use a VotingClassifier to combine their predictions.\n",
    "\n",
    "We fit the pipeline on the training set using fit(X_train, y_train) and evaluate its accuracy on the test set using predict(X_test) and accuracy_score(y_test, y_pred).\n",
    "\n",
    "Note that in the VotingClassifier, we set voting='hard' which means that the final prediction is the majority vote of the predicted classes by the individual classifiers.\n",
    "\n",
    "Possible improvements to this pipeline could include tuning the hyperparameters of the individual classifiers and the VotingClassifier, using a different method for handling missing values or encoding categorical features, or trying different types of classifiers in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c355a5-fdf2-4429-88f6-0ac6adc6a522",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
