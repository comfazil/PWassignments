{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16d0eaf-f94c-4f2b-87a8-57ab9ce8e217",
   "metadata": {},
   "source": [
    "## Q1. How does bagging reduce overfitting in decision trees?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a1d187-6254-4bd3-9183-e83dbee3a746",
   "metadata": {},
   "source": [
    "Bagging reduces overfitting in decision trees by reducing the variance of the model. Bagging generates multiple subsets of the training data by bootstrapping and trains multiple independent models on these subsets. These models are combined to produce a final output. The variance of the model is reduced because the combined output of multiple models is less sensitive to changes in the training data than a single model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645b2df1-a00c-4b7e-a32f-07879dfe1771",
   "metadata": {},
   "source": [
    "## Q2. What are the advantages and disadvantages of using different types of base learners in bagging?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b63ac1e-3ba9-4871-8de2-e7804aa56494",
   "metadata": {},
   "source": [
    "The advantages and disadvantages of using different types of base learners in bagging are:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Different base learners may capture different aspects of the data, leading to a more diverse ensemble.\n",
    "- The ensemble may be more robust to noise and outliers if the base learners are complementary.\n",
    "- Using more complex base learners may improve the accuracy of the ensemble.\n",
    "\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Using different types of base learners may increase the computational complexity of the ensemble.\n",
    "- Using more complex base learners may increase the risk of overfitting, especially if the base learners are highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d27862-cd97-476a-bdab-3d1905d640fc",
   "metadata": {},
   "source": [
    "## Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebc98b9-2b04-44bf-953b-45ca725966ce",
   "metadata": {},
   "source": [
    "The choice of base learner affects the bias-variance tradeoff in bagging. A low-bias, high-variance base learner such as a decision tree benefits more from bagging than a high-bias, low-variance base learner such as a linear model. Bagging reduces the variance of the model by combining the predictions of multiple independent models. If the base learner has high variance, the reduction in variance due to bagging is more pronounced, leading to a larger improvement in the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dcca8a-d124-4efb-83bc-a22144d8d34e",
   "metadata": {},
   "source": [
    "## Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c616a2-2f22-4de7-b653-ff3fc73b983d",
   "metadata": {},
   "source": [
    "Yes, bagging can be used for both classification and regression tasks. The difference lies in the output of the ensemble. In classification tasks, the output of the ensemble is typically the majority vote of the individual models. In regression tasks, the output of the ensemble is typically the mean or median of the individual model predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a4cae-948a-4c04-bfbb-110223be2c7a",
   "metadata": {},
   "source": [
    "## Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04780461-7457-4034-8691-fc2f5c961289",
   "metadata": {},
   "source": [
    "The ensemble size in bagging refers to the number of models included in the ensemble. The larger the ensemble size, the more diverse the models, and the lower the variance of the ensemble. However, adding more models beyond a certain point may lead to diminishing returns or even degrade the performance of the ensemble due to overfitting. The optimal number of models in the ensemble depends on the dataset and the base learner. In practice, ensemble sizes between 10 and 100 are common."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db76a3cf-d3bb-4ad2-8f2c-f942244677fa",
   "metadata": {},
   "source": [
    "## Q6. Can you provide an example of a real-world application of bagging in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972957ea-57cb-40d4-859b-d3d82f0bba3d",
   "metadata": {},
   "source": [
    "A real-world application of bagging in machine learning is in the field of finance, where it is used to predict stock prices. Bagging is used to generate multiple independent models based on historical stock data and market trends. The ensemble is used to make predictions about the future direction of stock prices. Bagging can help to reduce the variance of the predictions and improve the accuracy of the model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0245709d-e385-4b8e-bd8c-732e03a49469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
