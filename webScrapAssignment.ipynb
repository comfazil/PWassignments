{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db6d2719-6fe4-4d81-bac7-8795709e2989",
   "metadata": {},
   "source": [
    "## Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c599b048-9c6d-4a81-83d4-edbe9a91f866",
   "metadata": {},
   "source": [
    "Web scraping refers to the process of automatically extracting data from websites.The extracted data can then be saved and analyzed for various purposes.\n",
    "\n",
    "Web scraping is used for a variety of reasons, such as:\n",
    "\n",
    "Market research: Companies can use web scraping to gather information about competitors, pricing, customer reviews, and other data points to inform their own marketing strategies.\n",
    "\n",
    "Academic research: Researchers can use web scraping to collect data from various sources, such as social media platforms or news websites, to analyze trends, public opinion, and other variables.\n",
    "\n",
    "Data analysis: Web scraping can be used to collect large amounts of data quickly and efficiently, which can then be analyzed and used to inform business decisions, such as pricing strategies or inventory management.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf69f0a-71af-4f49-b557-46e1e2734dd3",
   "metadata": {},
   "source": [
    "## Q2. What are the different methods used for Web Scraping ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556bc5bd-ab8b-4a65-9593-cf22fa1547c3",
   "metadata": {},
   "source": [
    "1 - Manual Scripting\n",
    "2 - HTML Parsing\n",
    "3 - Web Scrapping Tools\n",
    "4 - APIs\n",
    "5 - Machine Learning Techniques\n",
    "6 - Headless Browser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb5f372-4102-4134-b008-e097186119ef",
   "metadata": {},
   "source": [
    "## Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d54deb7-763a-44af-ad1f-3a3b6d2ac43e",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library that is used for web scraping purposes. It provides a set of tools for parsing HTML and XML documents, allowing users to extract data from web pages in a structured and efficient way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4430fb70-a600-4ed3-bc9b-0459241a8f8b",
   "metadata": {},
   "source": [
    "## Q4. Why is flask used in this Web Scraping project ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10ca50-4b18-4f57-b2b2-4413f41f64fc",
   "metadata": {},
   "source": [
    "Flask is a lightweight web framework that is commonly used for building web applications and APIs. In the context of a web scraping project, Flask can be used to create a web interface that allows users to interact with the scraped data.\n",
    "Flask is a useful tool for a web scraping project because it provides an easy and flexible way to create a web interface for scraped data, while also integrating well with other Python libraries commonly used in web scraping projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae4453e-a83e-4d2c-a63f-e51fe52497fa",
   "metadata": {},
   "source": [
    "## Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0788f373-3566-472f-8a2c-4806152ff212",
   "metadata": {},
   "source": [
    "Web scraping programmes can be run on virtual machines (instances) provided by Amazon EC2 (Elastic Compute Cloud), a scalable cloud computing service. Users can select the best configuration for their unique web scraping requirements by configuring EC2 instances with a variety of operating systems and specifications.\n",
    "\n",
    "Scraped data can be kept in Amazon S3 (Simple Storage Service), a cloud storage service. For web scraping projects that need data storage, S3 offers a robust and scalable storage solution that can be accessed from anywhere in the globe.\n",
    "\n",
    "Scalable and economical web scraping script execution is possible using Amazon Lambda, a serverless computing service. When certain events occur, such as fresh data being added to a database or a file being uploaded to S3, lambda functions can be automatically executed.\n",
    "\n",
    "For monitoring and logging purposes, web scraping scripts running on EC2 instances or Lambda functions can be monitored using Amazon CloudWatch. Users may immediately identify problems by using CloudWatch to monitor metrics like CPU utilisation, network traffic, and memory consumption.\n",
    "\n",
    "A managed database service called Amazon RDS (Relational Database Service) can be used to store data that has been scraped in a structured manner. RDS offers support for a number of database engines, including MySQL, PostgreSQL, and Oracle, enabling users to select the best solution for their particular web scraping project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe06189e-8eee-48c1-bec4-ffb33d256945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
