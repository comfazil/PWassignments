{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f548990-9637-48fd-b9e6-16b69b6d9d52",
   "metadata": {},
   "source": [
    "## Q1. What is Gradient Boosting Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77861e97-32d5-4c5b-90d1-a90c43fcf8e0",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a popular machine learning algorithm for regression problems. It is an extension of the gradient boosting algorithm that is used for classification problems. In Gradient Boosting Regression, an ensemble of weak regression models is built sequentially, where each model is trained on the residuals of the previous model. The final prediction is obtained by summing the predictions of all the models in the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d7479f-a40f-455b-8306-e478b684bf30",
   "metadata": {},
   "source": [
    "## Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a291c21-ad8f-41a3-948e-2c66c13165e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 10909.328331339491\n",
      "R^2: 0.6476776030863363\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    \n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        residuals = y\n",
    "        for i in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            predictions = tree.predict(X)\n",
    "            residuals = residuals - self.learning_rate * predictions\n",
    "            self.estimators.append(tree)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(len(X))\n",
    "        for tree in self.estimators:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# Example usage\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=0.5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print('MSE:', mse)\n",
    "print('R^2:', r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b877d991-6bb0-4870-9e13-5f1dc11ebf56",
   "metadata": {},
   "source": [
    "## Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c00d4-c6d8-474a-b1b8-acb1606ea09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print('Best params:', best_params)\n",
    "\n",
    "model = GradientBoostingRegressor(**best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print('MSE:', mse)\n",
    "print('R^2:', r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d391a-b2fd-4e15-b008-6ce40e9d32e3",
   "metadata": {},
   "source": [
    "## Q4. What is a weak learner in Gradient Boosting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c38f9-adb7-4531-a461-637b16e4fa85",
   "metadata": {},
   "source": [
    "A weak learner in Gradient Boosting is a simple model that is designed to do slightly better than random guessing on the training set. In Gradient Boosting, the weak learners are typically decision trees with a small depth. These weak learners are trained sequentially, with each subsequent learner trying to correct the errors of the previous ones. The final prediction is obtained by summing the predictions of all the weak learners in the ensemble, each weighted by a scalar value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba836d3c-784a-4952-a4f6-565df74f9ffe",
   "metadata": {},
   "source": [
    "## Q5. What is the intuition behind the Gradient Boosting algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff978397-5974-40b4-81d1-0e05397e4e88",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm is to create an ensemble of weak learners that can together make strong predictions. The algorithm works by training each weak learner on the residual errors of the previous ones, where the residual errors are the differences between the true labels and the predictions made by the previous weak learners. The algorithm tries to iteratively reduce the residual errors by adding new weak learners to the ensemble, each one correcting the errors of the previous ones. By summing the predictions of all the weak learners in the ensemble, we obtain the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d14d590-1dcb-4466-b7d9-7364a235b51d",
   "metadata": {},
   "source": [
    "## Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e01641-71e7-4995-a85c-95c55aaaab4b",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners in the following steps:\n",
    "\n",
    "- Initialize the ensemble with a weak learner, such as a decision tree with a small depth.\n",
    "- Train the weak learner on the training data and make predictions.\n",
    "- Calculate the residual errors as the differences between the true labels and the predictions made by the weak learner.\n",
    "- Train a new weak learner on the residual errors and make predictions.\n",
    "- Combine the predictions of the two weak learners by adding them up, each one weighted by a scalar value.\n",
    "- Repeat steps 3 to 5 until the desired number of weak learners is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f9211f-ecf0-4837-84d2-7a0667d14adb",
   "metadata": {},
   "source": [
    "## Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2154d0-72d8-47f2-86bd-7f0bdf665228",
   "metadata": {},
   "source": [
    "The mathematical intuition of the Gradient Boosting algorithm can be constructed in the following steps:\n",
    "\n",
    "1. Let X be the input features, y be the true labels, and F be the model we want to learn.\n",
    "2. Initialize the model with a constant value, such as the mean of y.\n",
    "3. Compute the negative gradient of the loss function with respect to the current model, which represents the residual errors.\n",
    "4. Train a weak learner, such as a decision tree, to fit the residual errors.\n",
    "5. Add the weak learner to the model by multiplying its predictions by a small scalar value, called the learning rate, and adding them to the current model.\n",
    "6. Repeat steps 3 to 5 until the desired number of weak learners is reached.\n",
    "7. The final model is the sum of all the weak learners, each one weighted by the learning rate.\n",
    "\n",
    "\n",
    "The steps above represent the intuition behind Gradient Boosting for regression. For classification problems, the same steps can be followed with some modifications, such as using a different loss function and modifying the way the weak learners are combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7af8b1-8f50-4d98-bf9c-486abf0be34c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
